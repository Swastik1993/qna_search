{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to COVID-19 Q&A deployment Project layout covid_qna \u251c\u2500 data/ # Stores the data and .py file for downloading the data. \u2502 \u2514\u2500 data_download.py # Code for downloading all the lucene-indexed data. \u2502 \u251c\u2500 src/ # Contains all the code necessary for execution of the application. \u2502 \u2514\u2500 main.py # Has the definition of every API that is calling the modules class. \u2502 \u2514\u2500 init_models.py # Used for downloading and initializing all the models. \u2502 \u2514\u2500 modules.py # Contains all the function definitions for the various modules. \u2502 \u251c\u2500 results/ # Stores the results of each query. \u2502 \u2514\u2500 rich_text.json # Example API rersponse as json for rich-text query. \u2502 \u2514\u2500 detailed_text.json # Example API rersponse as json for detailed-text query. \u2502 \u2514\u2500 requirements.txt # List of all python packages that are used. \u2514\u2500 Dockerfile # Definition for building the docker image. \u2514\u2500 docker-compose.yaml # YAML definition of docker-compose for easy deployment. \u2514\u2500 README.md Pipeline details A simple workflow for the entire application can be summarized as follows: This application has utilized multiple modules each of which can be splitted and used seperately if required. If you would like to have a deeper understanding of how every module is working and the design choices at each and every step please click here or else you can use the navbar to navigate to the Implementation Details section. Languages used (app running on FastAPI backend) This version has been modified slightly from the original version so that it can run without using GPU acceleration. * If you would like to view the actual code with GPU acceleration that uses both Pytorch and Tensorflow please visit the master branch of the bitbucket repo . This code is similar to the no_gpu branch of the repo . * Please make sure to check the all files and comments and make necessary changes if required before building with docker. * This is the python code for deployment purpose only. Once the indexes are built, the lucene index folder should be updated. This code will NOT build the lucene indexes . Prerequisites If you are planning to build and test (deploy) with docker then Docker CE or Docker EE needs to be installed. (Instuctions for installing docker are also covered in this documentation.) If you planning to not use docker then please use Linux environment to test. As of now this application will NOT run natively on windows OS. Python version 3.6 and above is supported. Corresponding pip version should also be present. Testing The application was mainly designed as a quick response action item to the COVID-19 situation. Since there is a huge volume of unstructured data that needed to be mined this application was developed. The testing is done mainly on the CORD-19 research dataset which has over 50,000 coronavirus and COVID-19 research articles apart from other articles as well. Total count of articles in the dataset as of date 17/09/2020 is 266175 (including duplicates). You can visit this site for check the application. Steps for building and executing Before starting make sure you have a high speed internet connection and enough data because the code will download almost 10GB of data altogether while it is being built. Also make sure you have enough disk space because once the application/images are built the total size of both the application/images running in the system would be almost 20GB. (In case of docker the intermediate images are also built in this process that may be of even larger sizes, so make sure to clean the system at the end once the images are built.) In case you are building with docker (recommended approach) please follow the below instructions else you can skip this section. Docker build guide Before going ahead with the docker build please remember that you will have to edit the module.py file once before the docker build instruction. Simply open the file and uncomment line 71 and comment out line 72 and save the file. That's all! For Windows systems If you are using windows chances are you might have to get in touch with the IT team to get Docker installed. This README is written considering you have admin rights or you are installing on a Windows server. Please visit this official docker url and then click on 'Docker Desktop for Windows' followed by 'Download from Docker Hub' . Finally click on 'Get Stable' to download Docker Desktop Community Edition (However if you are installing on a windows server using Enterprise edition is recommended). After that simply install docker by follow the on screen instructions. Then unzip the archive and extract the files. Open terminal and change your path to the covid_qna directory. Once inside the directory you will need to download the lucene indexed data. cd covid_qna/ In order to download the lucene indexed data for covid-19 articles navigate to the data folder and execute the python script. cd data/ pip install tqdm python data_download.py On running these commands the lucene index data in a .tar.gz file (4.5GB approx) will be downloaded. Navigate back to the root directory and run the following commands in sequence in order to build and run the docker images. (Please check if you have edited the module.py file) cd .. docker-compose build docker-compose up -d The first command will build the docker images. It will take sometime (about 20-30mins depending on your internet speed and system processing speed.)* Once everything is done the application should start. In order to access the UI for the python APIs then please visit http://localhost:5000/docs/ . For Linux systems Open your terminal and type the following commanda to get docker and docker-compose installed. <!-- For installing Docker --> sudo apt install docker.io docker --version <!-- If you are unable to see the docker version then use the below command to add the current user to docker group after that logout and log back in and then re-run the above command to check the version. --> sudo usermod -aG docker $USER <!-- For installing docker-compose --> sudo apt install docker-compose docker-compose version Then unzip the archive and extract the files. Open terminal and change your path to the covid_qna directory. Once inside the directory you will need to download the lucene indexed data. cd covid_qna/ In order to download the lucene indexed data for covid-19 articles navigate to the data folder and execute the python script. cd data/ pip install tqdm python data_download.py On running these commands the lucene index data in a .tar.gz file (4.5GB approx) will be downloaded. Navigate back to the root directory and run the following commands in sequence in order to build and run the docker images. (Please check if you have edited the module.py file) cd .. docker-compose build docker-compose up -d The first command will build the docker images. It will take sometime (about 20-30mins depending on your internet speed and system processing speed.)* Once everything is done the application should start. In order to access the UI for the python APIs then please visit http://localhost:5000/docs/ . Removing temporary images Once the docker images are built you can choose to run the prune command to remove any intermediate/unused images that were created in the process of building the images. In order to do that please use the following command. docker system prune On running this command it will prompt to confirm by [y/N], press y to clean the intermediate/unused images Without using docker As of now the application does not work on windows because the Java virtual environment does not gets initialized. Hence the only approach is using Linux systems. For Linux systems Open your terminal and type the following commanda to get the latest package updates and install Java, Python virtual environment and a few other necessary packages. sudo apt-get update sudo apt-get install -y openjdk-11-jdk sudo apt-get install -y curl build-essential gcc python3 python3-venv Now create a new virtual environment and activate it. Then unzip the archive and extract the files. python3 -m venv ml_env source ml_env/bin/activate unzip covid_qna.zip Now change your path to the covid_qna directory. Once inside the directory you will need to install the required python packages and then download the lucene indexed data. cd covid_qna/ pip install -r requirements.txt In order to download the lucene indexed data for covid-19 articles navigate to the data folder and execute the python script. cd data/ python data_download.py On running these commands the lucene index data in a .tar.gz file (4.5GB approx) will be downloaded. * Then you need to untar the compressed file for the python app. tar -xvzf lucene-index-cord19.tar.gz Navigate back to the root directory and change to the src directory. After that run the commands to download all the models and run the application. cd ../src/ python init_models.py uvicorn --host 0.0.0.0 app:app The first command will download all the required models and prep them for the application. Once everything is done the application should start. In order to access the UI for the python APIs then please visit http://localhost:8000/docs/ . Bugs Application does not run natively on Windows OS. While using the rich-text/detailed-text API if there are no matches found then the response is given as an internal server error. It does not mean that the code has crashed. It is simply because it has not found anything to respond back. This bug will be fixed in the next version. Future scope The objective is to build a NLP engine so powerful that it can act by itself as a search engine over any kind of documented large scale data. Extend scope to include direct parsing of PDF or other documents and build indexes by itself. Add more flexibilty to support indexes from Elasticsearch and Apache Solr. I hope there would not be any problems while running the application. However, in case of any issues please reach out to me .","title":"Home"},{"location":"#welcome-to-covid-19-qa-deployment","text":"","title":"Welcome to COVID-19 Q&amp;A deployment"},{"location":"#project-layout","text":"covid_qna \u251c\u2500 data/ # Stores the data and .py file for downloading the data. \u2502 \u2514\u2500 data_download.py # Code for downloading all the lucene-indexed data. \u2502 \u251c\u2500 src/ # Contains all the code necessary for execution of the application. \u2502 \u2514\u2500 main.py # Has the definition of every API that is calling the modules class. \u2502 \u2514\u2500 init_models.py # Used for downloading and initializing all the models. \u2502 \u2514\u2500 modules.py # Contains all the function definitions for the various modules. \u2502 \u251c\u2500 results/ # Stores the results of each query. \u2502 \u2514\u2500 rich_text.json # Example API rersponse as json for rich-text query. \u2502 \u2514\u2500 detailed_text.json # Example API rersponse as json for detailed-text query. \u2502 \u2514\u2500 requirements.txt # List of all python packages that are used. \u2514\u2500 Dockerfile # Definition for building the docker image. \u2514\u2500 docker-compose.yaml # YAML definition of docker-compose for easy deployment. \u2514\u2500 README.md","title":"Project layout"},{"location":"#pipeline-details","text":"A simple workflow for the entire application can be summarized as follows: This application has utilized multiple modules each of which can be splitted and used seperately if required. If you would like to have a deeper understanding of how every module is working and the design choices at each and every step please click here or else you can use the navbar to navigate to the Implementation Details section.","title":"Pipeline details"},{"location":"#languages-used","text":"(app running on FastAPI backend) This version has been modified slightly from the original version so that it can run without using GPU acceleration. * If you would like to view the actual code with GPU acceleration that uses both Pytorch and Tensorflow please visit the master branch of the bitbucket repo . This code is similar to the no_gpu branch of the repo . * Please make sure to check the all files and comments and make necessary changes if required before building with docker. * This is the python code for deployment purpose only. Once the indexes are built, the lucene index folder should be updated. This code will NOT build the lucene indexes .","title":"Languages used"},{"location":"#prerequisites","text":"If you are planning to build and test (deploy) with docker then Docker CE or Docker EE needs to be installed. (Instuctions for installing docker are also covered in this documentation.) If you planning to not use docker then please use Linux environment to test. As of now this application will NOT run natively on windows OS. Python version 3.6 and above is supported. Corresponding pip version should also be present.","title":"Prerequisites"},{"location":"#testing","text":"The application was mainly designed as a quick response action item to the COVID-19 situation. Since there is a huge volume of unstructured data that needed to be mined this application was developed. The testing is done mainly on the CORD-19 research dataset which has over 50,000 coronavirus and COVID-19 research articles apart from other articles as well. Total count of articles in the dataset as of date 17/09/2020 is 266175 (including duplicates). You can visit this site for check the application.","title":"Testing"},{"location":"#steps-for-building-and-executing","text":"Before starting make sure you have a high speed internet connection and enough data because the code will download almost 10GB of data altogether while it is being built. Also make sure you have enough disk space because once the application/images are built the total size of both the application/images running in the system would be almost 20GB. (In case of docker the intermediate images are also built in this process that may be of even larger sizes, so make sure to clean the system at the end once the images are built.) In case you are building with docker (recommended approach) please follow the below instructions else you can skip this section.","title":"Steps for building and executing"},{"location":"#docker-build-guide","text":"Before going ahead with the docker build please remember that you will have to edit the module.py file once before the docker build instruction. Simply open the file and uncomment line 71 and comment out line 72 and save the file. That's all!","title":"Docker build guide"},{"location":"#for-windows-systems","text":"If you are using windows chances are you might have to get in touch with the IT team to get Docker installed. This README is written considering you have admin rights or you are installing on a Windows server. Please visit this official docker url and then click on 'Docker Desktop for Windows' followed by 'Download from Docker Hub' . Finally click on 'Get Stable' to download Docker Desktop Community Edition (However if you are installing on a windows server using Enterprise edition is recommended). After that simply install docker by follow the on screen instructions. Then unzip the archive and extract the files. Open terminal and change your path to the covid_qna directory. Once inside the directory you will need to download the lucene indexed data. cd covid_qna/ In order to download the lucene indexed data for covid-19 articles navigate to the data folder and execute the python script. cd data/ pip install tqdm python data_download.py On running these commands the lucene index data in a .tar.gz file (4.5GB approx) will be downloaded. Navigate back to the root directory and run the following commands in sequence in order to build and run the docker images. (Please check if you have edited the module.py file) cd .. docker-compose build docker-compose up -d The first command will build the docker images. It will take sometime (about 20-30mins depending on your internet speed and system processing speed.)* Once everything is done the application should start. In order to access the UI for the python APIs then please visit http://localhost:5000/docs/ .","title":"For Windows systems"},{"location":"#for-linux-systems","text":"Open your terminal and type the following commanda to get docker and docker-compose installed. <!-- For installing Docker --> sudo apt install docker.io docker --version <!-- If you are unable to see the docker version then use the below command to add the current user to docker group after that logout and log back in and then re-run the above command to check the version. --> sudo usermod -aG docker $USER <!-- For installing docker-compose --> sudo apt install docker-compose docker-compose version Then unzip the archive and extract the files. Open terminal and change your path to the covid_qna directory. Once inside the directory you will need to download the lucene indexed data. cd covid_qna/ In order to download the lucene indexed data for covid-19 articles navigate to the data folder and execute the python script. cd data/ pip install tqdm python data_download.py On running these commands the lucene index data in a .tar.gz file (4.5GB approx) will be downloaded. Navigate back to the root directory and run the following commands in sequence in order to build and run the docker images. (Please check if you have edited the module.py file) cd .. docker-compose build docker-compose up -d The first command will build the docker images. It will take sometime (about 20-30mins depending on your internet speed and system processing speed.)* Once everything is done the application should start. In order to access the UI for the python APIs then please visit http://localhost:5000/docs/ .","title":"For Linux systems"},{"location":"#removing-temporary-images","text":"Once the docker images are built you can choose to run the prune command to remove any intermediate/unused images that were created in the process of building the images. In order to do that please use the following command. docker system prune On running this command it will prompt to confirm by [y/N], press y to clean the intermediate/unused images","title":"Removing temporary images"},{"location":"#without-using-docker","text":"As of now the application does not work on windows because the Java virtual environment does not gets initialized. Hence the only approach is using Linux systems.","title":"Without using docker"},{"location":"#for-linux-systems_1","text":"Open your terminal and type the following commanda to get the latest package updates and install Java, Python virtual environment and a few other necessary packages. sudo apt-get update sudo apt-get install -y openjdk-11-jdk sudo apt-get install -y curl build-essential gcc python3 python3-venv Now create a new virtual environment and activate it. Then unzip the archive and extract the files. python3 -m venv ml_env source ml_env/bin/activate unzip covid_qna.zip Now change your path to the covid_qna directory. Once inside the directory you will need to install the required python packages and then download the lucene indexed data. cd covid_qna/ pip install -r requirements.txt In order to download the lucene indexed data for covid-19 articles navigate to the data folder and execute the python script. cd data/ python data_download.py On running these commands the lucene index data in a .tar.gz file (4.5GB approx) will be downloaded. * Then you need to untar the compressed file for the python app. tar -xvzf lucene-index-cord19.tar.gz Navigate back to the root directory and change to the src directory. After that run the commands to download all the models and run the application. cd ../src/ python init_models.py uvicorn --host 0.0.0.0 app:app The first command will download all the required models and prep them for the application. Once everything is done the application should start. In order to access the UI for the python APIs then please visit http://localhost:8000/docs/ .","title":"For Linux systems"},{"location":"#bugs","text":"Application does not run natively on Windows OS. While using the rich-text/detailed-text API if there are no matches found then the response is given as an internal server error. It does not mean that the code has crashed. It is simply because it has not found anything to respond back. This bug will be fixed in the next version.","title":"Bugs"},{"location":"#future-scope","text":"The objective is to build a NLP engine so powerful that it can act by itself as a search engine over any kind of documented large scale data. Extend scope to include direct parsing of PDF or other documents and build indexes by itself. Add more flexibilty to support indexes from Elasticsearch and Apache Solr. I hope there would not be any problems while running the application. However, in case of any issues please reach out to me .","title":"Future scope"},{"location":"detailed/","text":"Functions serving Rich Answers Note The main application is broken down into two sections. One is for the rich-text API that gives all the results along with a brief summary of them and another is the detailed-text API that gives the answer from a specific paper but a much more detailed answer not just a summary. This page will define all the function definition that are used for serving the detailed-text answers. The docstring for every function is also present in the code. These functions are defined in the BertSquad class in modules.py file for serving the rich-text API. class BertSquad: \"\"\" The main class which has all the fucntions defined for serving the different API calls. Functions: show_query(): show_document(): extract_scibert(): get_result_id(): cross_match(): show_sections(): highlight_paragraph(): show_results(): \"\"\" show_query() def show_query(self, query): \"\"\" Returns HTML format for the searched query. Parameters: query (str): the question based on which the answers need to be found. Returns: HTML text (str): returns the query with enclosed in some html styling. \"\"\" show_document() def show_document(self, idx, doc): \"\"\" Returns HTML format for the searched query. Parameters: idx (str): the index of the document. doc (str): the actual text of the document. Returns: HTML text (str): returns the document with the title, authors and id enclosed in some html styling. \"\"\" extract_scibert() def extract_scibert(self, text, tokenizer, model): \"\"\" Extracts the contextualized vectors for the given text/query and abstracts from SciBERT/BioBERT model for highlighting relevant paragraphs. Parameters: text (str): the text or query(question) for which the vectrs needs to be found. tokenizer (str): the tokenizer that is going to be used. model (str): the model that is going to be used. Returns: text_ids (tensor): the ids for the words as a tensor text_words (list): list of words for state vectors. state (tensor): the state vectors for the text as a tensor. \"\"\" get_result_id() def get_result_id(self, query, doc_id, searcher): \"\"\" Returns the document/search result that has been found for the given question and doc_id. Parameters: query (str): the given question which for which a doc_id is searched. doc_id (str): the document id on which the question is going to be searched. searcher (str): the searching function that is going to be used to search the doc_id for the question. Returns: hit (str): returns the entire document/result that has been found. \"\"\" cross_match() def cross_match(self, state1, state2): \"\"\" Computes the cosine similarity matrix between the query and each paragraph and returns the state vectors. Parameters: state1 (tensor): the query state vectors. state2 (tensor): the paragraph state vectors. Returns: sim (tensor): the tensor scores after calculating the cosine similarity metrics. \"\"\" show_sections() def show_sections(self, section, text): \"\"\" Returns HTML format for the searched query. Parameters: section (str): the entire docuemnt portion question based on which the answers need to be found. text (str): the text which will be from the section Returns: HTML text (str): returns the text with enclosed in some html styling. \"\"\" highlight_paragraph() def highlight_paragraph(self, ptext, rel_words, max_win=10): \"\"\" Highlights the relevant phrases in each paragraph. Any word that had a high similarity to each of the query words is considered relevant. Given these words, we highlight a window of 10 surrounding each of them. Parameters: ptext (list): the list of words that are going to be highlighted. rel_words (list): the scores of the words that will be used for highlighting. max_win (int): the window size based on which the number of words will be highlighted. By default value is 10. Returns: para (str): returns the highlighted text enclosed in some html styling. \"\"\" show_results() def show_results(self, question, doc_id): \"\"\" Returns HTML format for the searched query. Parameters: question (str): query (str): the question based on which the answers need to be found. doc_id (str): the document id on which the question is going to be searched. Returns: data (dict): returns the id, title, metadata and text from the document in a dictionary. \"\"\"","title":"Detailed Answers"},{"location":"detailed/#functions-serving-rich-answers","text":"Note The main application is broken down into two sections. One is for the rich-text API that gives all the results along with a brief summary of them and another is the detailed-text API that gives the answer from a specific paper but a much more detailed answer not just a summary. This page will define all the function definition that are used for serving the detailed-text answers. The docstring for every function is also present in the code. These functions are defined in the BertSquad class in modules.py file for serving the rich-text API. class BertSquad: \"\"\" The main class which has all the fucntions defined for serving the different API calls. Functions: show_query(): show_document(): extract_scibert(): get_result_id(): cross_match(): show_sections(): highlight_paragraph(): show_results(): \"\"\"","title":"Functions serving Rich Answers"},{"location":"detailed/#show_query","text":"def show_query(self, query): \"\"\" Returns HTML format for the searched query. Parameters: query (str): the question based on which the answers need to be found. Returns: HTML text (str): returns the query with enclosed in some html styling. \"\"\"","title":"show_query()"},{"location":"detailed/#show_document","text":"def show_document(self, idx, doc): \"\"\" Returns HTML format for the searched query. Parameters: idx (str): the index of the document. doc (str): the actual text of the document. Returns: HTML text (str): returns the document with the title, authors and id enclosed in some html styling. \"\"\"","title":"show_document()"},{"location":"detailed/#extract_scibert","text":"def extract_scibert(self, text, tokenizer, model): \"\"\" Extracts the contextualized vectors for the given text/query and abstracts from SciBERT/BioBERT model for highlighting relevant paragraphs. Parameters: text (str): the text or query(question) for which the vectrs needs to be found. tokenizer (str): the tokenizer that is going to be used. model (str): the model that is going to be used. Returns: text_ids (tensor): the ids for the words as a tensor text_words (list): list of words for state vectors. state (tensor): the state vectors for the text as a tensor. \"\"\"","title":"extract_scibert()"},{"location":"detailed/#get_result_id","text":"def get_result_id(self, query, doc_id, searcher): \"\"\" Returns the document/search result that has been found for the given question and doc_id. Parameters: query (str): the given question which for which a doc_id is searched. doc_id (str): the document id on which the question is going to be searched. searcher (str): the searching function that is going to be used to search the doc_id for the question. Returns: hit (str): returns the entire document/result that has been found. \"\"\"","title":"get_result_id()"},{"location":"detailed/#cross_match","text":"def cross_match(self, state1, state2): \"\"\" Computes the cosine similarity matrix between the query and each paragraph and returns the state vectors. Parameters: state1 (tensor): the query state vectors. state2 (tensor): the paragraph state vectors. Returns: sim (tensor): the tensor scores after calculating the cosine similarity metrics. \"\"\"","title":"cross_match()"},{"location":"detailed/#show_sections","text":"def show_sections(self, section, text): \"\"\" Returns HTML format for the searched query. Parameters: section (str): the entire docuemnt portion question based on which the answers need to be found. text (str): the text which will be from the section Returns: HTML text (str): returns the text with enclosed in some html styling. \"\"\"","title":"show_sections()"},{"location":"detailed/#highlight_paragraph","text":"def highlight_paragraph(self, ptext, rel_words, max_win=10): \"\"\" Highlights the relevant phrases in each paragraph. Any word that had a high similarity to each of the query words is considered relevant. Given these words, we highlight a window of 10 surrounding each of them. Parameters: ptext (list): the list of words that are going to be highlighted. rel_words (list): the scores of the words that will be used for highlighting. max_win (int): the window size based on which the number of words will be highlighted. By default value is 10. Returns: para (str): returns the highlighted text enclosed in some html styling. \"\"\"","title":"highlight_paragraph()"},{"location":"detailed/#show_results","text":"def show_results(self, question, doc_id): \"\"\" Returns HTML format for the searched query. Parameters: question (str): query (str): the question based on which the answers need to be found. doc_id (str): the document id on which the question is going to be searched. Returns: data (dict): returns the id, title, metadata and text from the document in a dictionary. \"\"\"","title":"show_results()"},{"location":"implementation/","text":"Implementation Details This section covers in detail the implementation details along with the reasons and some advantages of using the various modules. Also why the specific modules were chosen and built in that manner. Lucene Indexes Now looking into the details there are many elments that work independently to make this application possible. The data which is provided by Semantic Scholar and Allen Institute of AI is actually a compressed file containing multiple json files in different folders. Each json file has the schema similar to that of a research paper which is something similar to this schema. \"root\": { \"paper_id\":string\"[paper id]\" \"metadata\":{ \"title\":string\"[paper title]\" \"authors\":[...] \"abstract\":string\"[abstract]\" } \"body_text\": [ 0:{ \"text\":string\"[section text]\" \"cite_spans\":[...] \"section\":string\"[section name]\" \"ref_spans\":[] } 1:{...} 2:{...} 3:{...} 4:{...} ... ] \"ref_entries\": { \"FIGREF0\":{...} } \"back_matter\":[] \"bib_entries\": { \"BIBREF0\": { \"title\":string\"[reference paper title]\" \"authors\":[...] \"year\":int[year] \"venue\":string\"[text]\" \"volume\":string\"[text]\" \"issn\":string\"\" \"pages\":string\"[text]\" \"other_ids\":{...} } \"BIBREF1\":{...} \"BIBREF2\":{...} \"BIBREF3\":{...} \"BIBREF4\":{...} ... } } Since most of the actual data is present in the abstract and body_text we decided to index every document's data with the abstract and body_text for quicker access. In order to index the data we have used Apache Lucene (Apache Solr and Elasticsearch also uses Lucene indexes in the background). The figure below shows the basic internal structure of an index. The data in a segment is represented abstractly rather than as a realistic representation of the actual data structure. The code for creating lucene index is written in Java which is beyond the scope of this process. The below figure represents how indexes are created and retreived while searching is done. Bystoring the documents in lucene indexes our search works at a complexity of O(1). In order to read more about Lucene indexes you can visit this link . BERT model Once the data we will be using pyserini to query and fetch data from the lucene indexes and use BERT for question answering that is trained on SQuAD dataset. This is required because we would some model that can understand the intent of the user's question and serve only the best matching results. For this BERT is one of the best pre-trained models that is available. It is designed to work natively on pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. Now the SQuAD dataset is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Thus the BERT model which is finetuned on SQuAD data gives us with almost all the right tools to start building our application. The image below is a representation of how the model is working. The great thing with BERT is that it's embedding is very powerful hence it is very suitable for many NLP tasks. You can read more about how BERT is used for question answering on the SQuAD data in this paper . For our code we used the bert-large model finetuned on squad data as a pretrained model in PyTorch from the Hugging Face Transformers library. BART model After this we used the BART algorithm. BART is a denoising autoencoder for pretraining sequence-to-sequence models. As described in the paper , BART uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token as described in it's architecture. As a result, BART performs well on multiple tasks like abstractive dialogue, question answering and summarization. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. Therefore we decided to use BART for generating the final rich-text as a short summary of all the answers. We used the bart-large model which is trained on the CNN/Daily Mail data set and it has been the canonical data set for summarization work. The data sets consist of news articles and abstractive summaries written by humans. Even though the model that is trained on CNN/Daily Mail data should not be used for generating COVID-19 research articles summary the results obtained has mixed performance but it does provide a coherent summary which is factually correct hence we decided to stick with it(for now). BioBERT model At this stage we had achieved our objective of building a search engine on top of research articles but we decided to push it further so that we can also get details from a specific paper for the given question. For this we could have simply utilized the existing BERT model to get out objective and it would have worked out fine. But even in BERT the ability of understanding the medical data was not present. We needed something that could give more context to understand biomedical data so that the results from the specific document really looks to have the detailed answer of the question. For these reasons we decided to use the BioBERT model . This is a pre-trained bio-medical language representation model for various bio-medical text mining tasks. Tasks such as NER from Bio-medical data, relation extraction, question & answer in the biomedical field can be solved by utilizing this model. Since the model itself is a domain specific language representation model based on BERT and pre-trained on arge-scale biomedical corpora of PMC and PubMed. We mainly used BioBERT for summarization and selecting text that had the most similar matches to the tokens in the question however that is not all. BioBERT can actually be utilized for biomedical NER (which is beyond the scope of this process), and relation extraction for build relations in graphs. Our use case in this case is served by using BioBERT for question answering.","title":"Implementation Details"},{"location":"implementation/#implementation-details","text":"This section covers in detail the implementation details along with the reasons and some advantages of using the various modules. Also why the specific modules were chosen and built in that manner.","title":"Implementation Details"},{"location":"implementation/#lucene-indexes","text":"Now looking into the details there are many elments that work independently to make this application possible. The data which is provided by Semantic Scholar and Allen Institute of AI is actually a compressed file containing multiple json files in different folders. Each json file has the schema similar to that of a research paper which is something similar to this schema. \"root\": { \"paper_id\":string\"[paper id]\" \"metadata\":{ \"title\":string\"[paper title]\" \"authors\":[...] \"abstract\":string\"[abstract]\" } \"body_text\": [ 0:{ \"text\":string\"[section text]\" \"cite_spans\":[...] \"section\":string\"[section name]\" \"ref_spans\":[] } 1:{...} 2:{...} 3:{...} 4:{...} ... ] \"ref_entries\": { \"FIGREF0\":{...} } \"back_matter\":[] \"bib_entries\": { \"BIBREF0\": { \"title\":string\"[reference paper title]\" \"authors\":[...] \"year\":int[year] \"venue\":string\"[text]\" \"volume\":string\"[text]\" \"issn\":string\"\" \"pages\":string\"[text]\" \"other_ids\":{...} } \"BIBREF1\":{...} \"BIBREF2\":{...} \"BIBREF3\":{...} \"BIBREF4\":{...} ... } } Since most of the actual data is present in the abstract and body_text we decided to index every document's data with the abstract and body_text for quicker access. In order to index the data we have used Apache Lucene (Apache Solr and Elasticsearch also uses Lucene indexes in the background). The figure below shows the basic internal structure of an index. The data in a segment is represented abstractly rather than as a realistic representation of the actual data structure. The code for creating lucene index is written in Java which is beyond the scope of this process. The below figure represents how indexes are created and retreived while searching is done. Bystoring the documents in lucene indexes our search works at a complexity of O(1). In order to read more about Lucene indexes you can visit this link .","title":"Lucene Indexes"},{"location":"implementation/#bert-model","text":"Once the data we will be using pyserini to query and fetch data from the lucene indexes and use BERT for question answering that is trained on SQuAD dataset. This is required because we would some model that can understand the intent of the user's question and serve only the best matching results. For this BERT is one of the best pre-trained models that is available. It is designed to work natively on pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. Now the SQuAD dataset is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Thus the BERT model which is finetuned on SQuAD data gives us with almost all the right tools to start building our application. The image below is a representation of how the model is working. The great thing with BERT is that it's embedding is very powerful hence it is very suitable for many NLP tasks. You can read more about how BERT is used for question answering on the SQuAD data in this paper . For our code we used the bert-large model finetuned on squad data as a pretrained model in PyTorch from the Hugging Face Transformers library.","title":"BERT model"},{"location":"implementation/#bart-model","text":"After this we used the BART algorithm. BART is a denoising autoencoder for pretraining sequence-to-sequence models. As described in the paper , BART uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT). The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token as described in it's architecture. As a result, BART performs well on multiple tasks like abstractive dialogue, question answering and summarization. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. Therefore we decided to use BART for generating the final rich-text as a short summary of all the answers. We used the bart-large model which is trained on the CNN/Daily Mail data set and it has been the canonical data set for summarization work. The data sets consist of news articles and abstractive summaries written by humans. Even though the model that is trained on CNN/Daily Mail data should not be used for generating COVID-19 research articles summary the results obtained has mixed performance but it does provide a coherent summary which is factually correct hence we decided to stick with it(for now).","title":"BART model"},{"location":"implementation/#biobert-model","text":"At this stage we had achieved our objective of building a search engine on top of research articles but we decided to push it further so that we can also get details from a specific paper for the given question. For this we could have simply utilized the existing BERT model to get out objective and it would have worked out fine. But even in BERT the ability of understanding the medical data was not present. We needed something that could give more context to understand biomedical data so that the results from the specific document really looks to have the detailed answer of the question. For these reasons we decided to use the BioBERT model . This is a pre-trained bio-medical language representation model for various bio-medical text mining tasks. Tasks such as NER from Bio-medical data, relation extraction, question & answer in the biomedical field can be solved by utilizing this model. Since the model itself is a domain specific language representation model based on BERT and pre-trained on arge-scale biomedical corpora of PMC and PubMed. We mainly used BioBERT for summarization and selecting text that had the most similar matches to the tokens in the question however that is not all. BioBERT can actually be utilized for biomedical NER (which is beyond the scope of this process), and relation extraction for build relations in graphs. Our use case in this case is served by using BioBERT for question answering.","title":"BioBERT model"},{"location":"rich/","text":"Functions serving Rich Answers Note The main application is broken down into two sections. One is for the rich-text API that gives all the results along with a brief summary of them and another is the detailed-text API that gives the answer from a specific paper but a much more detailed answer not just a summary. This page will define all the function definition that are used for serving the rich-text answers. The docstring for every function is also present in the code. These functions are defined in the BertSquad class in modules.py file for serving the rich-text API. class BertSquad: \"\"\" The main class which has all the fucntions defined for serving the different API calls. Functions: reconstructText(): makeBERTSQuADPrediction(): searchAbstracts(): displayResults(): getrecord(): pubMedSearch(): medrxivSearch(): searchDatabase(): \"\"\" reconstructText() def reconstructText(self, tokens, start=0, stop=-1): \"\"\" Returns the text after merging the specific tokens from start to end to form a readable sentence. Parameters: tokens (list): list of tokens. start (int): starting index position. stop (int): stopping index position. Returns: newList (list): list of small sentences. \"\"\" makeBERTSQuADPrediction() def makeBERTSQuADPrediction(self, document, question): \"\"\" Returns all the predicted possible answers found for the question on a specific document set using the BERT model running on pytorch. Parameters: document (str): the abstract documents on which the highlighting is required. question (str): the question based on which the answers need to be found. Returns: ans (dict): dictionary of the answers found by BERT along with their confidence scores. \"\"\" searchAbstracts() def searchAbstracts(self, hit_dictionary, question): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: hit_dictionary (dict): dictionary containing all the documents that have been found as a match for the given question by searching through all the lucene indexes. question (str): the question based on which the answers need to be found. Returns: abstractResults (dict): dictionary of all the relevant results along with their BERT highlighted answers with confidence scores. \"\"\" displayResults() def displayResults(self, hit_dictionary, answers, question): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: hit_dictionary (dict): dictionary containing all the documents that have been found as a match for the given question by searching through all the lucene indexes. answers (dict): dictionary of all the relevant results along with their BERT highlighted answers with confidence scores. question (str): the question based on which the answers need to be found. Returns: summ (str): generated BART summary based on all the answers. warning_HTML (str): text for the UI. [can be replaced with literally anything. Not very important.] df (dataframe converted to json): json data containing all the relevant fields necessary for the UI layer. \"\"\" getrecord() def getrecord(self, id, db): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: id (str): id for the document which needs to be fetched. db (str): the database which needs to be searched. Returns: rec (object): returns the object of the text for the id. \"\"\" pubMedSearch() def pubMedSearch(self, terms, db='pubmed', mindate='2019/12/01'): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: terms (str): the keywords on which the data will be searched. db (str): by default searches the pubmed data if required can be used for similar data search. mindate (date as str): by default it is 1st December the date from which the search will be performed. Returns: record_db (dict): the pubmed article(s) which has been found. \"\"\" medrxivSearch() def medrxivSearch(self, query, n_pages=1): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: query (str): the keywords on which the data will be searched. n_pages (int): the number of pages of information that needs to be fetched. Returns: results (dict): the medarxiv article(s) which has been found. \"\"\" searchDatabase() def searchDatabase(self, question, keywords): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: question (str): the question based on which the answers need to be found. keywords (list): list of additional keywords for the question to give more context for the answer to search. Returns: displayResults(hit_dictionary, answers, question): returns the data from displayResults() call. summ (str): generated BART summary based on all the answers. warning_HTML (str): text for the UI. [can be replaced with literally anything. Not very important.] df (dataframe converted to json): json data containing all the relevant fields necessary for the UI layer. \"\"\"","title":"Rich Answers"},{"location":"rich/#functions-serving-rich-answers","text":"Note The main application is broken down into two sections. One is for the rich-text API that gives all the results along with a brief summary of them and another is the detailed-text API that gives the answer from a specific paper but a much more detailed answer not just a summary. This page will define all the function definition that are used for serving the rich-text answers. The docstring for every function is also present in the code. These functions are defined in the BertSquad class in modules.py file for serving the rich-text API. class BertSquad: \"\"\" The main class which has all the fucntions defined for serving the different API calls. Functions: reconstructText(): makeBERTSQuADPrediction(): searchAbstracts(): displayResults(): getrecord(): pubMedSearch(): medrxivSearch(): searchDatabase(): \"\"\"","title":"Functions serving Rich Answers"},{"location":"rich/#reconstructtext","text":"def reconstructText(self, tokens, start=0, stop=-1): \"\"\" Returns the text after merging the specific tokens from start to end to form a readable sentence. Parameters: tokens (list): list of tokens. start (int): starting index position. stop (int): stopping index position. Returns: newList (list): list of small sentences. \"\"\"","title":"reconstructText()"},{"location":"rich/#makebertsquadprediction","text":"def makeBERTSQuADPrediction(self, document, question): \"\"\" Returns all the predicted possible answers found for the question on a specific document set using the BERT model running on pytorch. Parameters: document (str): the abstract documents on which the highlighting is required. question (str): the question based on which the answers need to be found. Returns: ans (dict): dictionary of the answers found by BERT along with their confidence scores. \"\"\"","title":"makeBERTSQuADPrediction()"},{"location":"rich/#searchabstracts","text":"def searchAbstracts(self, hit_dictionary, question): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: hit_dictionary (dict): dictionary containing all the documents that have been found as a match for the given question by searching through all the lucene indexes. question (str): the question based on which the answers need to be found. Returns: abstractResults (dict): dictionary of all the relevant results along with their BERT highlighted answers with confidence scores. \"\"\"","title":"searchAbstracts()"},{"location":"rich/#displayresults","text":"def displayResults(self, hit_dictionary, answers, question): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: hit_dictionary (dict): dictionary containing all the documents that have been found as a match for the given question by searching through all the lucene indexes. answers (dict): dictionary of all the relevant results along with their BERT highlighted answers with confidence scores. question (str): the question based on which the answers need to be found. Returns: summ (str): generated BART summary based on all the answers. warning_HTML (str): text for the UI. [can be replaced with literally anything. Not very important.] df (dataframe converted to json): json data containing all the relevant fields necessary for the UI layer. \"\"\"","title":"displayResults()"},{"location":"rich/#getrecord","text":"def getrecord(self, id, db): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: id (str): id for the document which needs to be fetched. db (str): the database which needs to be searched. Returns: rec (object): returns the object of the text for the id. \"\"\"","title":"getrecord()"},{"location":"rich/#pubmedsearch","text":"def pubMedSearch(self, terms, db='pubmed', mindate='2019/12/01'): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: terms (str): the keywords on which the data will be searched. db (str): by default searches the pubmed data if required can be used for similar data search. mindate (date as str): by default it is 1st December the date from which the search will be performed. Returns: record_db (dict): the pubmed article(s) which has been found. \"\"\"","title":"pubMedSearch()"},{"location":"rich/#medrxivsearch","text":"def medrxivSearch(self, query, n_pages=1): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: query (str): the keywords on which the data will be searched. n_pages (int): the number of pages of information that needs to be fetched. Returns: results (dict): the medarxiv article(s) which has been found. \"\"\"","title":"medrxivSearch()"},{"location":"rich/#searchdatabase","text":"def searchDatabase(self, question, keywords): \"\"\" Returns all the possible answers found for the given question after searching through the hits found document set. Parameters: question (str): the question based on which the answers need to be found. keywords (list): list of additional keywords for the question to give more context for the answer to search. Returns: displayResults(hit_dictionary, answers, question): returns the data from displayResults() call. summ (str): generated BART summary based on all the answers. warning_HTML (str): text for the UI. [can be replaced with literally anything. Not very important.] df (dataframe converted to json): json data containing all the relevant fields necessary for the UI layer. \"\"\"","title":"searchDatabase()"}]}